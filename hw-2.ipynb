{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from random import choice, uniform\n",
    "from time import sleep, time, ctime\n",
    "from tqdm import tqdm_notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy_list():\n",
    "    url = 'https://free-proxy-list.com'\n",
    "    html = requests.get(url, headers={\n",
    "        'User-Agent': 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0; FunWebProducts)'\n",
    "    }).text\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    proxy_list = []\n",
    "    for elem in soup.find_all('a', attrs={'href': re.compile(\"/proxyserver/.+\")}):\n",
    "        proxy_list.append(elem['alt'])\n",
    "    return proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useragents = open('useragents.txt', 'r', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://sitespy.ru/my-ip'\n",
    "\n",
    "def get_ip(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    ip = soup.find('span', class_='ip').text.strip()\n",
    "    ua = soup.find('span', class_='ip').find_next_sibling('span').text.strip()\n",
    "    return ip, ua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(soup, urls_list):\n",
    "    for elem in soup.find_all('a', attrs={'class': 'item-description-title-link'}):\n",
    "        if elem['href'] not in urls_list:\n",
    "            urls_list.add(elem['href'])\n",
    "    return urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url, proxy, useragents):\n",
    "    proxies = {'https': 'https://' + choice(proxy)}\n",
    "    useragent = {'User-Agent': choice(useragents)}\n",
    "    \n",
    "    try:\n",
    "        html = requests.get('https://www.avito.ru' + url, headers=useragent, proxies=proxies).text\n",
    "    except:\n",
    "        print(url)\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return url, soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_urls(urls_list, proxy, useragents, start):\n",
    "    url = 'https://www.avito.ru/rossiya/komnaty?p='    \n",
    "    while len(urls_list) <= 10000:\n",
    "        \n",
    "        for i in tqdm_notebook(range(start, 1000)):\n",
    "            sleep(uniform(2, 5))\n",
    "            soup = get_soup('https://www.avito.ru' + url + str(i), proxy, useragents)[1]\n",
    "            if not soup:\n",
    "                continue\n",
    "            urls_list = get_urls(soup, urls_list)\n",
    "    \n",
    "    return urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = get_proxy_list()\n",
    "urls_list = collect_urls(urls_list, proxy, useragents, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.json', 'w', encoding='utf-8') as fw:\n",
    "    json.dump(list(urls_list), fw, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_find(soup, url, tag, class_):\n",
    "    try:\n",
    "        res = re.sub('  +', ' ', soup.find(tag, class_=class_).text.strip().replace('\\n\\n', '\\n')) + '\\n--\\n'\n",
    "        return res\n",
    "    except:\n",
    "        print('soup_find {} fails for {}'.format(class_, url))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_page_data(url, soup):\n",
    "    title = soup_find(soup, url, 'span', 'title-info-title-text')\n",
    "    time = 'Дата размещения: ' + soup_find(soup, url, 'div', 'title-info-metadata-item')\n",
    "    price = 'Цена: ' + soup_find(soup, url, 'span', 'price-value-string js-price-value-string')\n",
    "    seller = 'Продавец: ' + soup_find(soup, url, 'div', 'seller-info js-seller-info')\n",
    "    info = 'Характеристики: ' + soup_find(soup, url, 'div', 'item-view-block')\n",
    "    address = soup_find(soup, url, 'div', 'item-map-location').replace('Скрыть карту', '')\n",
    "    text = 'Описание: ' + soup_find(soup, url, 'div', 'item-description')\n",
    "    \n",
    "    with open(os.getcwd() + '/corpus/' + url.replace('/', '--') + '.txt', 'w', encoding='utf-8') as fw:\n",
    "        data = re.sub('\\n +', '\\n', title + time + price + seller + info + address + text)\n",
    "        cleaned =  data.replace('\\n\\n', '\\n')\n",
    "        fw.write(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_visited():\n",
    "    urls_visited = set()\n",
    "    \n",
    "    for elem in os.listdir(os.getcwd() + '/corpus'):\n",
    "        urls_visited.add(elem.replace('--', '/').replace('.txt', ''))\n",
    "    \n",
    "    return urls_visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ads(urls, useragents, proxy): \n",
    "    urls_visited = get_urls_visited()\n",
    "    \n",
    "    for url in tqdm_notebook(urls): \n",
    "        if url in urls_visited:\n",
    "            continue\n",
    "        \n",
    "        sleep(uniform(2, 5))\n",
    "        \n",
    "        url_soup = get_soup(url, useragents, proxy)\n",
    "        if url_soup:\n",
    "            save_page_data(url_soup[0], url_soup[1])\n",
    "            urls_visited.add(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.json', 'r', encoding='utf-8') as f:\n",
    "    urls = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = get_proxy_list()\n",
    "get_ads(urls, proxy, useragents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(os.getcwd() + '/corpus/'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
